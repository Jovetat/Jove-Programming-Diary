
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import numpy as np
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    precision_recall_fscore_support,
    classification_report,
    confusion_matrix
)
import warnings
from openpyxl import load_workbook
import argparse

warnings.filterwarnings('ignore')

# ä¸­æ–‡å­—ä½“è®¾ç½®
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'FangSong', 'KaiTi', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False
matplotlib.rcParams['font.size'] = 10


def process_classification_report(y_true, y_pred, all_labels):
    """å¤„ç†åˆ†ç±»æŠ¥å‘Šï¼Œåˆ é™¤accuracyè¡Œå¹¶å¤„ç†é›¶æ ·æœ¬æƒ…å†µ"""

    # è·å–åˆ†ç±»æŠ¥å‘Šå­—å…¸
    report_dict = classification_report(y_true, y_pred, labels=all_labels, zero_division=0, output_dict=True)

    # åˆ é™¤ accuracy è¡Œ
    if 'accuracy' in report_dict:
        del report_dict['accuracy']

    # è½¬æ¢ä¸ºDataFrame
    report_df = pd.DataFrame(report_dict).transpose()

    # é‡å‘½åsupportåˆ—ä¸ºtotal_samples
    report_df = report_df.rename(columns={'support': 'total_samples'})

    # å¤„ç†é›¶æ ·æœ¬æƒ…å†µï¼šsupportä¸º0æ—¶ï¼Œprecisionã€recallã€f1ç”¨"-"ä»£æ›¿
    zero_support_mask = (report_df['total_samples'] == 0)

    for col in ['precision', 'recall', 'f1-score']:
        if col in report_df.columns:
            report_df.loc[zero_support_mask, col] = '-'

    # æ ¼å¼åŒ–å…¶ä»–æ•°å€¼åˆ—ä¸º4ä½å°æ•°ï¼ˆæ’é™¤å·²ç»æ˜¯"-"çš„åˆ—å’Œtotal_samplesï¼‰
    for col in report_df.columns:
        if col != 'total_samples':
            # åªå¯¹æ•°å€¼ç±»å‹çš„åˆ—è¿›è¡Œæ ¼å¼åŒ–
            numeric_mask = pd.to_numeric(report_df[col], errors='coerce').notna()
            if numeric_mask.any():
                report_df.loc[numeric_mask, col] = pd.to_numeric(report_df.loc[numeric_mask, col]).round(4)

    return report_df


def print_classification_report_custom(y_true, y_pred, all_labels, label_width=15):
    """è‡ªå®šä¹‰æ‰“å°åˆ†ç±»æŠ¥å‘Šï¼Œä¸åŒ…å«accuracyè¡Œ"""

    report_dict = classification_report(y_true, y_pred, labels=all_labels, zero_division=0, output_dict=True)

    # åˆ é™¤accuracyè¡Œ
    filtered_report_dict = {k: v for k, v in report_dict.items() if k != 'accuracy'}

    # æ‰“å°è¡¨å¤´
    print(f"{'':>{label_width}} {'precision':>9} {'recall':>9} {'f1-score':>9} {'support':>9}")
    print()

    # æ‰“å°å„ç±»åˆ«æŒ‡æ ‡
    for label in all_labels:
        if label in filtered_report_dict:
            metrics = filtered_report_dict[label]
            support = int(metrics['support'])

            # å¦‚æœsupportä¸º0ï¼Œæ˜¾ç¤º"-"
            if support == 0:
                print(f"{label:>{label_width}} {'-':>9} {'-':>9} {'-':>9} {support:>9}")
            else:
                print(
                    f"{label:>{label_width}} {metrics['precision']:>9.2f} {metrics['recall']:>9.2f} {metrics['f1-score']:>9.2f} {support:>9}")

    print()
    # æ˜¾ç¤ºå¹³å‡å€¼
    for avg_type in ['macro avg', 'weighted avg']:
        if avg_type in filtered_report_dict:
            metrics = filtered_report_dict[avg_type]
            support = int(metrics['support'])
            print(
                f"{avg_type:>{label_width}} {metrics['precision']:>9.2f} {metrics['recall']:>9.2f} {metrics['f1-score']:>9.2f} {support:>9}")


# é¢†åŸŸåˆ†ç±»è¯„ä¼°
def evaluate_domain_classification(df, results_dict):
    print("\n" + "=" * 80)
    print("ä»»åŠ¡1ï¼šé¢†åŸŸåˆ†ç±»è¯„ä¼°")
    print("=" * 80)

    y_true = df['raw_domain']
    y_pred = df['domain']

    # åŸºç¡€ç»Ÿè®¡
    print(f"\næ•°æ®æ€»é‡: {len(df)} æ¡")
    print(f"ç±»åˆ«æ•°é‡: {y_true.nunique()} ä¸ª ({', '.join(sorted(y_true.unique()))})")

    # æ•´ä½“æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)


    print(f"\nã€æ•´ä½“æŒ‡æ ‡ã€‘")
    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f} ({precision * 100:.2f}%)")
    print(f"å¬å›ç‡ (Recall): {recall:.4f} ({recall * 100:.2f}%)")
    print(f"f1 (F1): {f1:.4f} ({f1 * 100:.2f}%)")


    # å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    print(f"\nã€å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ã€‘")
    # è·å–æ‰€æœ‰å‡ºç°çš„æ ‡ç­¾ï¼ˆçœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾çš„å¹¶é›†ï¼‰
    all_labels = sorted(set(y_true.unique()) | set(y_pred.unique()))

    # æ‰“å°è‡ªå®šä¹‰çš„åˆ†ç±»æŠ¥å‘Šï¼ˆä¸åŒ…å«accuracyè¡Œï¼‰
    print_classification_report_custom(y_true, y_pred, all_labels, label_width=15)

    # å¤„ç†DataFrameä¿å­˜
    report_df = process_classification_report(y_true, y_pred, all_labels)

    # ç¼“å­˜é¢†åŸŸåˆ†ç±»-è¯¦ç»†æŒ‡æ ‡, ä¿å­˜è¿›sheet
    results_dict['é¢†åŸŸåˆ†ç±»-è¯¦ç»†æŒ‡æ ‡'] = report_df

    # é”™è¯¯åˆ†æ
    print(f"\nã€é”™è¯¯æ ·æœ¬ç»Ÿè®¡ã€‘(æŒ‰æ•°é‡é™åº)")
    error_df = df[y_true != y_pred]
    if len(error_df) > 0:
        error_stats = error_df.groupby(['raw_domain', 'domain']).size().reset_index(name='é”™è¯¯æ•°é‡')
        error_stats = error_stats.sort_values('é”™è¯¯æ•°é‡', ascending=False)

        # ä¿å­˜é”™è¯¯ç»Ÿè®¡åˆ°Excel
        results_dict['é¢†åŸŸåˆ†ç±»-é”™è¯¯ç»Ÿè®¡'] = error_stats

        print(f"æ€»é”™è¯¯æ•°: {len(error_df)} æ¡ (é”™è¯¯ç‡: {len(error_df) / len(df) * 100:.2f}%)\n")
        for idx, row in error_stats.iterrows():
            print(f"{row['raw_domain']} â†’ {row['domain']}: {row['é”™è¯¯æ•°é‡']}ä¸ª")
    else:
        print("å®Œç¾åˆ†ç±»ï¼æ— é”™è¯¯æ ·æœ¬ã€‚")
        # å³ä½¿æ²¡æœ‰é”™è¯¯ï¼Œä¹Ÿä¿å­˜ä¸€ä¸ªç©ºçš„DataFrame
        results_dict['é¢†åŸŸåˆ†ç±»-é”™è¯¯ç»Ÿè®¡'] = pd.DataFrame(columns=['raw_domain', 'domain', 'é”™è¯¯æ•°é‡'])
    # å¯è§†åŒ–
    visualize_domain_classification(y_true, y_pred)


def visualize_domain_classification(y_true, y_pred):
    """é¢†åŸŸåˆ†ç±»å¯è§†åŒ–"""

    # å›¾1ï¼šæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    fig, ax = plt.subplots(figsize=(10, 8))

    # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨çœŸå®+é¢„æµ‹çš„å¹¶é›†ï¼Œä¸classification_reportä¿æŒä¸€è‡´
    labels = sorted(set(y_true.unique()) | set(y_pred.unique()))
    cm = confusion_matrix(y_true, y_pred, labels=labels)

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels,
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'}, ax=ax)

    ax.set_title('é¢†åŸŸåˆ†ç±»æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('é¢„æµ‹é¢†åŸŸ', fontsize=13, fontweight='bold')
    ax.set_ylabel('çœŸå®é¢†åŸŸ', fontsize=13, fontweight='bold')

    ax.set_xticklabels(ax.get_xticklabels())
    ax.set_yticklabels(ax.get_yticklabels())

    plt.tight_layout()
    plt.show()

    # å›¾2ï¼šå„ç±»åˆ«æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾
    # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨ç›¸åŒçš„æ ‡ç­¾é›†åˆ
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, labels=labels, zero_division=0
    )

    fig, ax = plt.subplots(figsize=(12, 7))

    x = np.arange(len(labels))
    width = 0.25

    bars1 = ax.bar(x - width, precision, width, label='Precision', color='#E74C3C', alpha=0.8)
    bars2 = ax.bar(x, recall, width, label='Recall', color='#3498DB', alpha=0.8)
    bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='#2ECC71', alpha=0.8)

    ax.set_xlabel('é¢†åŸŸ', fontsize=13, fontweight='bold')
    ax.set_ylabel('åˆ†æ•°', fontsize=13, fontweight='bold')
    ax.set_title('å„é¢†åŸŸåˆ†ç±»æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend(fontsize=11)
    ax.set_ylim(0, 1.1)
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2., height,
                    f'{height:.2f}',
                    ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.show()

    # å›¾3ï¼šé¢„æµ‹åˆ†å¸ƒ vs çœŸå®åˆ†å¸ƒå¯¹æ¯”
    fig, ax = plt.subplots(figsize=(12, 7))

    true_counts = y_true.value_counts().reindex(labels, fill_value=0)
    pred_counts = y_pred.value_counts().reindex(labels, fill_value=0)

    x = np.arange(len(labels))
    width = 0.35

    bars1 = ax.bar(x - width / 2, true_counts.values, width, label='çœŸå®åˆ†å¸ƒ',
                   color='#66C2A5', alpha=0.8, edgecolor='black', linewidth=1)
    bars2 = ax.bar(x + width / 2, pred_counts.values, width, label='é¢„æµ‹åˆ†å¸ƒ',
                   color='#FC8D62', alpha=0.8, edgecolor='black', linewidth=1)

    ax.set_xlabel('é¢†åŸŸ', fontsize=13, fontweight='bold')
    ax.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=13, fontweight='bold')
    ax.set_title('é¢„æµ‹åˆ†å¸ƒ vs çœŸå®åˆ†å¸ƒå¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend(fontsize=11)
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2., height,
                    f'{int(height)}',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')

    plt.tight_layout()
    plt.show()


# é¢†åŸŸ-æ„å›¾è”åˆåˆ†ç±»è¯„ä¼°
def evaluate_domain_intent_classification(df, results_dict):
    print("\n" + "=" * 80)
    print("ä»»åŠ¡2ï¼šé¢†åŸŸ-æ„å›¾è”åˆåˆ†ç±»è¯„ä¼°")
    print("=" * 80)

    # åˆ›å»ºç»„åˆæ ‡ç­¾
    y_true = df['raw_domain'] + '-' + df['raw_intent']
    y_pred = df['domain'] + '-' + df['intent']

    # åŸºç¡€ç»Ÿè®¡
    print(f"\næ•°æ®æ€»é‡: {len(df)} æ¡")
    print(f"ç»„åˆç±»åˆ«æ•°é‡: {y_true.nunique()} ä¸ª")

    # æ•´ä½“æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    print(f"\nã€æ•´ä½“æŒ‡æ ‡ã€‘")
    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f} ({precision * 100:.2f}%)")
    print(f"å¬å›ç‡ (Recall): {recall:.4f} ({recall * 100:.2f}%)")
    print(f"f1 (F1): {f1:.4f} ({f1 * 100:.2f}%)")

    # å„ç»„åˆç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    print(f"\nã€å„ç»„åˆç±»åˆ«è¯¦ç»†æŒ‡æ ‡ã€‘")

    # è·å–æ‰€æœ‰å‡ºç°çš„æ ‡ç­¾ï¼ˆçœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾çš„å¹¶é›†ï¼‰
    all_labels = sorted(set(y_true.unique()) | set(y_pred.unique()))

    # æ‰“å°è‡ªå®šä¹‰çš„åˆ†ç±»æŠ¥å‘Šï¼ˆä¸åŒ…å«accuracyè¡Œï¼‰
    print_classification_report_custom(y_true, y_pred, all_labels, label_width=25)

    # å¤„ç†DataFrameä¿å­˜
    report_df = process_classification_report(y_true, y_pred, all_labels)

    results_dict['é¢†åŸŸæ„å›¾è”åˆ-è¯¦ç»†æŒ‡æ ‡'] = report_df

    # åˆ†å±‚åˆ†æ
    print(f"\nã€åˆ†å±‚åˆ†æã€‘")
    # é¢†åŸŸå±‚é¢
    domain_correct = (df['raw_domain'] == df['domain']).sum()
    domain_accuracy = domain_correct / len(df)
    print(f"é¢†åŸŸæ­£ç¡®ç‡ï¼ˆä¸è€ƒè™‘æ„å›¾ï¼‰: {domain_accuracy:.4f} ({domain_accuracy * 100:.2f}%)")

    # å„é¢†åŸŸå†…çš„æ„å›¾æ­£ç¡®ç‡
    for domain in sorted(df['raw_domain'].unique()):
        domain_df = df[df['raw_domain'] == domain]
        if len(domain_df) > 0:
            # é¢†åŸŸé¢„æµ‹ä¹Ÿå¯¹çš„æƒ…å†µä¸‹ï¼Œæ„å›¾çš„æ­£ç¡®ç‡
            domain_correct_df = domain_df[domain_df['raw_domain'] == domain_df['domain']]
            if len(domain_correct_df) > 0:
                intent_correct = (domain_correct_df['raw_intent'] == domain_correct_df['intent']).sum()
                intent_accuracy = intent_correct / len(domain_correct_df)
                print(
                    f"  {domain}é¢†åŸŸï¼ˆé¢†åŸŸé¢„æµ‹æ­£ç¡®çš„å‰æä¸‹ï¼‰: æ„å›¾æ­£ç¡®ç‡ {intent_accuracy:.4f} ({intent_accuracy * 100:.2f}%)")

    print(f"\nå®Œå…¨åŒ¹é…ï¼ˆé¢†åŸŸ+æ„å›¾éƒ½æ­£ç¡®ï¼‰: {accuracy:.4f} ({accuracy * 100:.2f}%)")

    # é”™è¯¯åˆ†æ
    print(f"\nã€é”™è¯¯æ ·æœ¬ç»Ÿè®¡ã€‘(æŒ‰æ•°é‡é™åº)")
    error_df = df[y_true != y_pred]
    if len(error_df) > 0:
        error_df_with_labels = error_df.copy()
        error_df_with_labels['true_label'] = y_true[y_true != y_pred]
        error_df_with_labels['pred_label'] = y_pred[y_true != y_pred]

        error_stats = error_df_with_labels.groupby(['true_label', 'pred_label']).size().reset_index(name='é”™è¯¯æ•°é‡')
        error_stats = error_stats.sort_values('é”™è¯¯æ•°é‡', ascending=False)

        # ä¿å­˜é”™è¯¯ç»Ÿè®¡åˆ°Excel
        results_dict['é¢†åŸŸæ„å›¾è”åˆ-é”™è¯¯ç»Ÿè®¡'] = error_stats

        print(f"æ€»é”™è¯¯æ•°: {len(error_df)} æ¡ (é”™è¯¯ç‡: {len(error_df) / len(df) * 100:.2f}%)\n")
        print(f"å‰10ä¸ªæœ€å¸¸è§çš„é”™è¯¯ç±»å‹:")
        for idx, row in error_stats.head(10).iterrows():
            print(f"{row['true_label']} â†’ {row['pred_label']}: {row['é”™è¯¯æ•°é‡']}ä¸ª")
    else:
        print("å®Œç¾åˆ†ç±»ï¼æ— é”™è¯¯æ ·æœ¬ã€‚")
        # å³ä½¿æ²¡æœ‰é”™è¯¯ï¼Œä¹Ÿä¿å­˜ä¸€ä¸ªç©ºçš„DataFrame
        results_dict['é¢†åŸŸæ„å›¾è”åˆ-é”™è¯¯ç»Ÿè®¡'] = pd.DataFrame(columns=['true_label', 'pred_label', 'é”™è¯¯æ•°é‡'])

    # å¯è§†åŒ–
    visualize_domain_intent_classification(y_true, y_pred, df)


def visualize_domain_intent_classification(y_true, y_pred, df):
    """é¢†åŸŸ-æ„å›¾è”åˆåˆ†ç±»å¯è§†åŒ–"""

    # å›¾1ï¼šæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨çœŸå®+é¢„æµ‹çš„å¹¶é›†ï¼Œä¸classification_reportä¿æŒä¸€è‡´
    labels = sorted(set(y_true.unique()) | set(y_pred.unique()))

    if len(labels) <= 15:
        fig, ax = plt.subplots(figsize=(14, 12))

        cm = confusion_matrix(y_true, y_pred, labels=labels)

        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=labels, yticklabels=labels,
                    cbar_kws={'label': 'æ ·æœ¬æ•°é‡'}, ax=ax)

        ax.set_title('é¢†åŸŸ-æ„å›¾è”åˆåˆ†ç±»æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=13, fontweight='bold')
        ax.set_ylabel('çœŸå®ç±»åˆ«', fontsize=13, fontweight='bold')

        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=9)
        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=9)

        plt.tight_layout()
        plt.show()
    else:
        print(f"\n[æç¤º] ç±»åˆ«æ•°é‡è¾ƒå¤š({len(labels)}ä¸ª)ï¼Œæ··æ·†çŸ©é˜µè¿‡å¤§ï¼Œè·³è¿‡å®Œæ•´æ··æ·†çŸ©é˜µå¯è§†åŒ–")

    # å›¾2ï¼šå„ç»„åˆç±»åˆ«å‡†ç¡®ç‡æ’è¡Œ
    # ğŸ”§ ä½¿ç”¨ç›¸åŒçš„æ ‡ç­¾é›†åˆ
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    accuracy_list = []
    support_list = []

    for label in labels:
        # æ‰¾åˆ°è¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
        mask = (y_true == label)
        total = mask.sum()

        if total > 0:
            # è®¡ç®—è¯¥ç±»åˆ«é¢„æµ‹æ­£ç¡®çš„æ•°é‡
            correct = ((y_true == label) & (y_pred == label)).sum()
            accuracy = correct / total
        else:
            accuracy = 0

        accuracy_list.append(accuracy)
        support_list.append(total)

    # åˆ›å»ºDataFrameä¾¿äºæ’åº
    metrics_df = pd.DataFrame({
        'ç±»åˆ«': labels,
        'å‡†ç¡®ç‡': accuracy_list,
        'Support': support_list
    }).sort_values('å‡†ç¡®ç‡', ascending=True)  # å‡åºï¼Œä½åˆ†åœ¨å‰

    # åªæ˜¾ç¤ºå‰20ä¸ª
    display_df = metrics_df.tail(20) if len(metrics_df) > 20 else metrics_df

    fig, ax = plt.subplots(figsize=(10, max(8, len(display_df) * 0.4)))

    colors = ['#E74C3C' if score < 0.5 else '#F39C12' if score < 0.7 else '#2ECC71'
              for score in display_df['å‡†ç¡®ç‡']]

    bars = ax.barh(range(len(display_df)), display_df['å‡†ç¡®ç‡'], color=colors, alpha=0.8)

    ax.set_yticks(range(len(display_df)))
    ax.set_yticklabels(display_df['ç±»åˆ«'], fontsize=9)
    ax.set_xlabel('å‡†ç¡®ç‡', fontsize=12, fontweight='bold')
    ax.set_title(f'é¢†åŸŸ-æ„å›¾å‡†ç¡®ç‡æ’è¡Œ',
                 fontsize=14, fontweight='bold', pad=15)
    ax.set_xlim(0, 1.1)
    ax.grid(axis='x', alpha=0.3, linestyle='--')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, score, sup) in enumerate(zip(bars, display_df['å‡†ç¡®ç‡'], display_df['Support'])):
        ax.text(score + 0.02, i, f'{score:.3f} (n={sup})',
                va='center', fontsize=8)

    plt.tight_layout()
    plt.show()

    # å›¾3ï¼šæŒ‰é¢†åŸŸåˆ†ç»„çš„æ„å›¾å‡†ç¡®ç‡å¯¹æ¯”
    fig, ax = plt.subplots(figsize=(12, 7))

    domains = sorted(df['raw_domain'].unique())
    intent_accuracies = []

    for domain in domains:
        domain_df = df[df['raw_domain'] == domain]
        domain_correct_df = domain_df[domain_df['raw_domain'] == domain_df['domain']]
        if len(domain_correct_df) > 0:
            intent_correct = (domain_correct_df['raw_intent'] == domain_correct_df['intent']).sum()
            intent_accuracy = intent_correct / len(domain_correct_df)
        else:
            intent_accuracy = 0
        intent_accuracies.append(intent_accuracy)

    colors_domain = ['#66C2A5', '#FC8D62', '#8DA0CB', '#E78AC3']
    bars = ax.bar(range(len(domains)), intent_accuracies,
                  color=colors_domain[:len(domains)], alpha=0.8,
                  edgecolor='black', linewidth=1.2)

    ax.set_xticks(range(len(domains)))
    ax.set_xticklabels(domains, fontsize=12)
    ax.set_ylabel('æ„å›¾æ­£ç¡®ç‡', fontsize=13, fontweight='bold')
    ax.set_title('å„é¢†åŸŸå†…çš„æ„å›¾åˆ†ç±»å‡†ç¡®ç‡\n(ä»…ç»Ÿè®¡é¢†åŸŸé¢„æµ‹æ­£ç¡®çš„æ ·æœ¬)',
                 fontsize=14, fontweight='bold', pad=20)
    ax.set_ylim(0, 1.1)
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, acc) in enumerate(zip(bars, intent_accuracies)):
        domain_df = df[df['raw_domain'] == domains[i]]
        domain_correct_df = domain_df[domain_df['raw_domain'] == domain_df['domain']]
        ax.text(i, acc + 0.02, f'{acc:.2%}\n(n={len(domain_correct_df)})',
                ha='center', va='bottom', fontsize=10, fontweight='bold')

    plt.tight_layout()
    plt.show()



# å’Œè§£çŠ¶æ€åˆ†ç±»è¯„ä¼°
def evaluate_reconciliation_classification(df, results_dict):
    print("\n" + "=" * 80)
    print("ä»»åŠ¡3ï¼šå’Œè§£çŠ¶æ€åˆ†ç±»è¯„ä¼°")
    print("=" * 80)

    y_true = df['real_reconciliation']
    y_pred = df['reconciliation']

    # åŸºç¡€ç»Ÿè®¡
    print(f"\næ•°æ®æ€»é‡: {len(df)} æ¡")
    print(f"ç±»åˆ«æ•°é‡: {y_true.nunique()} ä¸ª ({', '.join(sorted(y_true.unique()))})")

    # æ•´ä½“æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)


    print(f"\nã€æ•´ä½“æŒ‡æ ‡ã€‘")
    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f} ({precision * 100:.2f}%)")
    print(f"å¬å›ç‡ (Recall): {recall:.4f} ({recall * 100:.2f}%)")
    print(f"f1 (F1): {f1:.4f} ({f1 * 100:.2f}%)")


    # å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    print(f"\nã€å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ã€‘")
    # è·å–æ‰€æœ‰å‡ºç°çš„æ ‡ç­¾ï¼ˆçœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾çš„å¹¶é›†ï¼‰
    all_labels = sorted(set(y_true.unique()) | set(y_pred.unique()))

    # æ‰“å°è‡ªå®šä¹‰çš„åˆ†ç±»æŠ¥å‘Šï¼ˆä¸åŒ…å«accuracyè¡Œï¼‰
    print_classification_report_custom(y_true, y_pred, all_labels, label_width=15)

    # å¤„ç†DataFrameä¿å­˜
    report_df = process_classification_report(y_true, y_pred, all_labels)

    # ç¼“å­˜é¢†åŸŸåˆ†ç±»-è¯¦ç»†æŒ‡æ ‡, ä¿å­˜è¿›sheet
    results_dict['å’Œè§£çŠ¶æ€åˆ†ç±»-è¯¦ç»†æŒ‡æ ‡'] = report_df

    # é”™è¯¯åˆ†æ
    print(f"\nã€é”™è¯¯æ ·æœ¬ç»Ÿè®¡ã€‘(æŒ‰æ•°é‡é™åº)")
    error_df = df[y_true != y_pred]
    if len(error_df) > 0:
        error_stats = error_df.groupby(['real_reconciliation', 'reconciliation']).size().reset_index(name='é”™è¯¯æ•°é‡')
        error_stats = error_stats.sort_values('é”™è¯¯æ•°é‡', ascending=False)

        # ä¿å­˜é”™è¯¯ç»Ÿè®¡åˆ°Excel
        results_dict['å’Œè§£çŠ¶æ€åˆ†ç±»-é”™è¯¯ç»Ÿè®¡'] = error_stats

        print(f"æ€»é”™è¯¯æ•°: {len(error_df)} æ¡ (é”™è¯¯ç‡: {len(error_df) / len(df) * 100:.2f}%)\n")
        for idx, row in error_stats.iterrows():
            print(f"{row['real_reconciliation']} â†’ {row['reconciliation']}: {row['é”™è¯¯æ•°é‡']}ä¸ª")
    else:
        print("å®Œç¾åˆ†ç±»ï¼æ— é”™è¯¯æ ·æœ¬ã€‚")
        # å³ä½¿æ²¡æœ‰é”™è¯¯ï¼Œä¹Ÿä¿å­˜ä¸€ä¸ªç©ºçš„DataFrame
        results_dict['å’Œè§£çŠ¶æ€åˆ†ç±»-é”™è¯¯ç»Ÿè®¡'] = pd.DataFrame(columns=['real_reconciliation', 'reconciliation', 'é”™è¯¯æ•°é‡'])
    # å¯è§†åŒ–
    visualize_reconciliation_classification(y_true, y_pred)

def visualize_reconciliation_classification(y_true, y_pred):
    """å’Œè§£çŠ¶æ€åˆ†ç±»å¯è§†åŒ–"""

    # å›¾1ï¼šæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    fig, ax = plt.subplots(figsize=(10, 8))

    # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨çœŸå®+é¢„æµ‹çš„å¹¶é›†ï¼Œä¸classification_reportä¿æŒä¸€è‡´
    labels = sorted(set(y_true.unique()) | set(y_pred.unique()))
    cm = confusion_matrix(y_true, y_pred, labels=labels)

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels,
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'}, ax=ax)

    ax.set_title('å’Œè§£çŠ¶æ€åˆ†ç±»æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('é¢„æµ‹å’Œè§£çŠ¶æ€', fontsize=13, fontweight='bold')
    ax.set_ylabel('çœŸå®å’Œè§£çŠ¶æ€', fontsize=13, fontweight='bold')

    ax.set_xticklabels(ax.get_xticklabels())
    ax.set_yticklabels(ax.get_yticklabels())

    plt.tight_layout()
    plt.show()

    # å›¾2ï¼šå„ç±»åˆ«æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾
    # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨ç›¸åŒçš„æ ‡ç­¾é›†åˆ
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, labels=labels, zero_division=0
    )

    fig, ax = plt.subplots(figsize=(12, 7))

    x = np.arange(len(labels))
    width = 0.25

    bars1 = ax.bar(x - width, precision, width, label='Precision', color='#E74C3C', alpha=0.8)
    bars2 = ax.bar(x, recall, width, label='Recall', color='#3498DB', alpha=0.8)
    bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='#2ECC71', alpha=0.8)

    ax.set_xlabel('å’Œè§£çŠ¶æ€', fontsize=13, fontweight='bold')
    ax.set_ylabel('åˆ†æ•°', fontsize=13, fontweight='bold')
    ax.set_title('å’Œè§£çŠ¶æ€åˆ†ç±»æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend(fontsize=11)
    ax.set_ylim(0, 1.1)
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2., height,
                    f'{height:.2f}',
                    ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.show()

    # å›¾3ï¼šé¢„æµ‹åˆ†å¸ƒ vs çœŸå®åˆ†å¸ƒå¯¹æ¯”
    fig, ax = plt.subplots(figsize=(12, 7))

    true_counts = y_true.value_counts().reindex(labels, fill_value=0)
    pred_counts = y_pred.value_counts().reindex(labels, fill_value=0)

    x = np.arange(len(labels))
    width = 0.35

    bars1 = ax.bar(x - width / 2, true_counts.values, width, label='çœŸå®åˆ†å¸ƒ',
                   color='#66C2A5', alpha=0.8, edgecolor='black', linewidth=1)
    bars2 = ax.bar(x + width / 2, pred_counts.values, width, label='é¢„æµ‹åˆ†å¸ƒ',
                   color='#FC8D62', alpha=0.8, edgecolor='black', linewidth=1)

    ax.set_xlabel('å’Œè§£çŠ¶æ€', fontsize=13, fontweight='bold')
    ax.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=13, fontweight='bold')
    ax.set_title('é¢„æµ‹åˆ†å¸ƒ vs çœŸå®åˆ†å¸ƒå¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend(fontsize=11)
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2., height,
                    f'{int(height)}',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')

    plt.tight_layout()
    plt.show()


def save_results_to_excel(file_path, results_dict, original_df):
    """å°†ç»“æœä¿å­˜åˆ°Excelæ–‡ä»¶çš„æ–°sheetä¸­"""
    print("\n" + "=" * 80)
    print("å¼€å§‹ä¿å­˜ç»“æœåˆ°Excel...")
    print("=" * 80)

    try:
        # å…ˆè¯»å–åŸå§‹æ•°æ®ï¼Œä¿ç•™æ‰€æœ‰ç°æœ‰sheet
        with pd.ExcelFile(file_path) as xls:
            existing_sheets = {}
            for sheet_name in xls.sheet_names:
                # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªsheetï¼ˆåŸå§‹æ•°æ®ï¼‰ï¼Œç”¨æ›´æ–°åçš„dfæ›¿æ¢
                if sheet_name == xls.sheet_names[0]:
                    existing_sheets[sheet_name] = original_df
                else:
                    existing_sheets[sheet_name] = pd.read_excel(xls, sheet_name=sheet_name)

        # ä½¿ç”¨ExcelWriterå†™å…¥ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            # å…ˆå†™å…¥åŸæœ‰çš„sheetsï¼ˆåŒ…æ‹¬æ›´æ–°åçš„åŸå§‹æ•°æ®ï¼‰
            for sheet_name, sheet_df in existing_sheets.items():
                sheet_df.to_excel(writer, sheet_name='åŸå§‹æ•°æ®', index=False)

            # å†™å…¥æ–°çš„ç»“æœsheets
            for sheet_name, result_df in results_dict.items():
                # é”™è¯¯ç»Ÿè®¡è¡¨ä¸éœ€è¦ç´¢å¼•
                if 'é”™è¯¯ç»Ÿè®¡' in sheet_name:
                    result_df.to_excel(writer, sheet_name=sheet_name, index=False)
                else:
                    result_df.to_excel(writer, sheet_name=sheet_name, index=True)
                print(f"âœ“ å·²ä¿å­˜è¡¨æ ¼: {sheet_name}")

        # ä½¿ç”¨openpyxlæ ¼å¼åŒ–æ•°å€¼åˆ—çš„å°æ•°ä½æ•°
        wb = load_workbook(file_path)

        for sheet_name in results_dict.keys():
            if sheet_name in wb.sheetnames:
                ws = wb[sheet_name]

                # é”™è¯¯ç»Ÿè®¡è¡¨ä¸éœ€è¦æ ¼å¼åŒ–
                if 'é”™è¯¯ç»Ÿè®¡' in sheet_name:
                    continue

                # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼‰ï¼Œç¬¬2åˆ—å¼€å§‹ï¼ˆç¬¬1åˆ—æ˜¯ç´¢å¼•ï¼‰
                for row in ws.iter_rows(min_row=2, min_col=2):
                    for cell in row:
                        # å¦‚æœæ˜¯æ•°å€¼ç±»å‹ï¼Œè®¾ç½®æ ¼å¼
                        if isinstance(cell.value, (int, float)) and cell.value is not None:
                            # æ£€æŸ¥åˆ—åæ˜¯å¦ä¸ºtotal_samples
                            col_name = ws.cell(1, cell.column).value
                            if col_name == 'total_samples':
                                cell.number_format = '0'
                            else:
                                cell.number_format = '0.0000'
                        # å¦‚æœæ˜¯"-"å­—ç¬¦ä¸²ï¼Œä¿æŒä¸å˜
                        elif cell.value == '-':
                            pass  # ä¿æŒä¸ºæ–‡æœ¬æ ¼å¼

        wb.save(file_path)
        wb.close()

        print("\n" + "=" * 80)
        print(f"âœ… æ‰€æœ‰ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {file_path}")
        print(f"   - åŸå§‹æ•°æ®å·²æ›´æ–°")
        print(f"   - æ–°å¢ {len(results_dict)} ä¸ªè¯„ä¼°æŒ‡æ ‡è¡¨")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ä¿å­˜è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("æç¤º: è¯·ç¡®ä¿Excelæ–‡ä»¶æ²¡æœ‰è¢«å…¶ä»–ç¨‹åºæ‰“å¼€")

def main():
    results_to_save = {}

    parser = argparse.ArgumentParser(description='æ¨¡å‹æ‰“æ ‡ç»“æœè¯„ä¼°è„šæœ¬')
    parser.add_argument('file_path', help='éœ€è¦è¯„ä¼°çš„æ–‡ä»¶è·¯å¾„(è¯„ä¼°ç»“æœåŒæ­¥ä¿å­˜åœ¨æºæ–‡ä»¶ä¸­)')
    parser.add_argument('--claim_point', action='store_true', help='è¯‰ç‚¹è¯„ä¼°')
    parser.add_argument('--reconciliation_state', action='store_true', help='å’Œè§£çŠ¶æ€è¯„ä¼°')

    args = parser.parse_args()
    if not args.claim_point and not args.reconciliation_state:
        print("\nâš ï¸  æœªæŒ‡å®šä»»ä½•è¯„ä¼°ä»»åŠ¡!")
        print("ä½¿ç”¨ --claim_point å¯ç”¨è¯‰ç‚¹è¯„ä¼°")
        print("ä½¿ç”¨ --reconciliation_state å¯ç”¨å’Œè§£çŠ¶æ€è¯„ä¼°")
        print("å¯ä»¥åŒæ—¶ä½¿ç”¨ä¸¤ä¸ªå‚æ•°")
    # è¯»å–æ•°æ®
    df = pd.read_excel(args.file_path)
    df = df[df['flag'] == 'qualified']
    print(df.head())


    print("=" * 80)
    print("æ•°æ®åŠ è½½å®Œæˆ")
    print("=" * 80)
    print(f"æ€»æ•°æ®é‡: {len(df)} æ¡")
    print(f"åˆ—å: {', '.join(df.columns.tolist())}")


    if args.claim_point:

        # ========== æ·»åŠ ä¸¤ä¸ªæ–°åˆ— ==========
        df['domain_correct'] = (df['raw_domain'] == df['domain']).astype(int)
        df['domain_intent_correct'] = ((df['raw_domain'] + '-' + df['raw_intent']) ==
                                       (df['domain'] + '-' + df['intent'])).astype(int)
        print(f"\nå‰5è¡Œæ•°æ®é¢„è§ˆ:")
        print(df[['raw_domain', 'domain', 'raw_intent', 'intent', 'domain_correct', 'domain_intent_correct']].head())
        # é¢†åŸŸåˆ†ç±»è¯„ä¼°
        evaluate_domain_classification(df, results_to_save)

        # é¢†åŸŸ-æ„å›¾è”åˆåˆ†ç±»è¯„ä¼°
        evaluate_domain_intent_classification(df, results_to_save)



    if args.reconciliation_state:
        df['reconciliation_correct'] = (df['real_reconciliation'] == df['reconciliation']).astype(int)
        print(f"\nå‰5è¡Œæ•°æ®é¢„è§ˆ:")
        print(df[['real_reconciliation', 'reconciliation',  'reconciliation_correct']].head())
        # å’Œè§£æ–¹æ¡ˆè¯„ä¼°
        evaluate_reconciliation_classification(df, results_to_save)

    # ä¿å­˜ç»“æœåˆ°Excelï¼ˆåŒ…æ‹¬æ›´æ–°åçš„åŸå§‹æ•°æ®å’Œè¯„ä¼°æŒ‡æ ‡è¡¨ï¼‰
    if results_to_save:
        save_results_to_excel(args.file_path, results_to_save, df)


# ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    main()
