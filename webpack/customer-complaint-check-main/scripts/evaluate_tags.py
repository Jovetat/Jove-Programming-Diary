"""
æ ‡ç­¾è¯„ä¼°è„šæœ¬ï¼ˆå‰ç«¯æœåŠ¡ç‰ˆï¼‰
åŸºäº data_analysis_2.0.py çš„å®Œæ•´è¯„ä¼°é€»è¾‘
====================================

åŠŸèƒ½ï¼š
- è¯„ä¼°è¯‰ç‚¹ã€è¯‰æ±‚ã€è§£å†³æ–¹æ¡ˆï¼ˆå¤šçº§æ ‡ç­¾ï¼‰
- è¯„ä¼°å’Œè§£çŠ¶æ€ï¼ˆå•çº§æ ‡ç­¾ï¼‰
- ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šï¼ˆ*_all_evaluation.xlsxï¼‰

è¾“å‡ºæ ¼å¼ä¸ data_analysis_2.0.py ä¿æŒä¸€è‡´
é€‚é…å‰ç«¯æœåŠ¡è°ƒç”¨
"""

import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import os
import sys


def evaluate_multi_level_tags(df, tag_config):
    """
    é€šç”¨å‡½æ•°ï¼šè¯„ä¼°å¤šçº§æ ‡ç­¾ï¼ˆdomain-intent-third_levelï¼‰
    
    å‚æ•°:
        df: DataFrameï¼ŒåŒ…å«é¢„æµ‹å’Œä¿®æ­£æ ‡ç­¾çš„æ•°æ®
        tag_config: å­—å…¸ï¼ŒåŒ…å«æ ‡ç­¾é…ç½®ä¿¡æ¯
    
    è¿”å›:
        dict: åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    tag_type_name = tag_config['tag_type_name']
    domain_col = tag_config['domain_col']
    intent_col = tag_config['intent_col']
    third_level_col = tag_config['third_level_col']
    corrected_domain_col = tag_config['corrected_domain_col']
    corrected_intent_col = tag_config['corrected_intent_col']
    corrected_third_level_col = tag_config['corrected_third_level_col']
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = [
        domain_col, intent_col, third_level_col,
        corrected_domain_col, corrected_intent_col, corrected_third_level_col
    ]
    
    for col in required_columns:
        if col not in df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {col}")
    
    # è¿‡æ»¤æ‰æ²¡æœ‰äººå·¥ä¿®æ­£æ ‡ç­¾çš„æ•°æ®
    df_filtered = df[df[corrected_domain_col].notna() & df[corrected_intent_col].notna()].copy()
    
    if len(df_filtered) == 0:
        print(f"è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ç»è¿‡äººå·¥ä¿®æ­£çš„{tag_type_name}æ ‡ç­¾æ•°æ®")
        return None
    
    print(f"ä½¿ç”¨ {len(df_filtered)} æ¡ç»è¿‡äººå·¥ä¿®æ­£çš„{tag_type_name}æ ‡ç­¾æ•°æ®è¿›è¡Œè¯„ä¼°")
    
    # å¤„ç†ç©ºå€¼
    df_filtered.loc[:, domain_col] = df_filtered[domain_col].fillna('')
    df_filtered.loc[:, intent_col] = df_filtered[intent_col].fillna('')
    df_filtered.loc[:, third_level_col] = df_filtered[third_level_col].fillna('')
    df_filtered.loc[:, corrected_domain_col] = df_filtered[corrected_domain_col].fillna('')
    df_filtered.loc[:, corrected_intent_col] = df_filtered[corrected_intent_col].fillna('')
    df_filtered.loc[:, corrected_third_level_col] = df_filtered[corrected_third_level_col].fillna('')
    
    # åˆ›å»ºäºŒçº§ç»„åˆæ ‡ç­¾ï¼ˆdomain-intentï¼‰
    full_tag_pred_col = f'{tag_type_name}_full_tag_pred'
    full_tag_true_col = f'{tag_type_name}_full_tag_true'
    df_filtered.loc[:, full_tag_pred_col] = (
        df_filtered[domain_col].astype(str) + '-' + df_filtered[intent_col].astype(str)
    )
    df_filtered.loc[:, full_tag_true_col] = (
        df_filtered[corrected_domain_col].astype(str) + '-' + df_filtered[corrected_intent_col].astype(str)
    )

    # åˆ›å»ºä¸‰çº§ç»„åˆæ ‡ç­¾ï¼ˆdomain-intent-third_levelï¼‰
    full_tag_third_pred_col = f'{tag_type_name}_full_tag_third_pred'
    full_tag_third_true_col = f'{tag_type_name}_full_tag_third_true'
    df_filtered.loc[:, full_tag_third_pred_col] = (
        df_filtered[domain_col].astype(str) + '-' +
        df_filtered[intent_col].astype(str) + '-' +
        df_filtered[third_level_col].astype(str)
    )
    df_filtered.loc[:, full_tag_third_true_col] = (
        df_filtered[corrected_domain_col].astype(str) + '-' +
        df_filtered[corrected_intent_col].astype(str) + '-' +
        df_filtered[corrected_third_level_col].astype(str)
    )
    
    # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
    overall_accuracy = (df_filtered[full_tag_pred_col] == df_filtered[full_tag_true_col]).mean()
    overall_three_level_accuracy = (df_filtered[full_tag_third_pred_col] == df_filtered[full_tag_third_true_col]).mean()
    domain_accuracy = (df_filtered[domain_col] == df_filtered[corrected_domain_col]).mean()
    intent_accuracy = (df_filtered[intent_col] == df_filtered[corrected_intent_col]).mean()
    third_level_accuracy = (df_filtered[third_level_col] == df_filtered[corrected_third_level_col]).mean()
    
    print(f"{tag_type_name}æ ‡ç­¾æ•´ä½“å‡†ç¡®ç‡ (domain-intent-third): {overall_three_level_accuracy:.4f}")
    print(f"{tag_type_name}æ ‡ç­¾æ•´ä½“å‡†ç¡®ç‡ (domain-intent): {overall_accuracy:.4f}")
    print(f"{tag_type_name}ä¸€çº§æ ‡ç­¾å‡†ç¡®ç‡ (domain): {domain_accuracy:.4f}")
    print(f"{tag_type_name}äºŒçº§æ ‡ç­¾å‡†ç¡®ç‡ (intent): {intent_accuracy:.4f}")
    print(f"{tag_type_name}ä¸‰çº§æ ‡ç­¾å‡†ç¡®ç‡ (third_level): {third_level_accuracy:.4f}")
    
    # äºŒçº§æ ‡ç­¾è¯¦ç»†è¯„ä¼°
    unique_true_labels = sorted(df_filtered[full_tag_true_col].unique())
    evaluation_results = []
    
    for label in unique_true_labels:
        label_mask = df_filtered[full_tag_true_col] == label
        label_count = label_mask.sum()
        
        if label_count == 0:
            continue
        
        y_true_binary = (df_filtered[full_tag_true_col] == label).astype(int)
        y_pred_binary = (df_filtered[full_tag_pred_col] == label).astype(int)
        
        precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
        recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
        f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
        
        parts = label.split('-')
        domain_part = parts[0] if len(parts) > 0 else ''
        intent_part = parts[1] if len(parts) > 1 else ''
        
        evaluation_results.append({
            'æ ‡ç­¾ç±»å‹': tag_type_name,
            'å®Œæ•´æ ‡ç­¾': label,
            'ä¸€çº§æ ‡ç­¾': domain_part,
            'äºŒçº§æ ‡ç­¾': intent_part,
            'æ ·æœ¬æ•°é‡': label_count,
            'å‡†ç¡®ç‡': round(recall, 4),
            'ç²¾ç¡®ç‡': round(precision, 4),
            'å¬å›ç‡': round(recall, 4),
            'F1åˆ†æ•°': round(f1, 4)
        })
    
    results_df_two_level = pd.DataFrame(evaluation_results)
    
    # ä¸‰çº§æ ‡ç­¾è¯¦ç»†è¯„ä¼°
    unique_true_labels_third = sorted(df_filtered[full_tag_third_true_col].unique())
    evaluation_results_third = []
    
    for label in unique_true_labels_third:
        label_mask = df_filtered[full_tag_third_true_col] == label
        label_count = label_mask.sum()
        
        if label_count == 0:
            continue
        
        y_true_binary = (df_filtered[full_tag_third_true_col] == label).astype(int)
        y_pred_binary = (df_filtered[full_tag_third_pred_col] == label).astype(int)
        
        precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
        recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
        f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
        
        parts = label.split('-')
        domain_part = parts[0] if len(parts) > 0 else ''
        intent_part = parts[1] if len(parts) > 1 else ''
        third_part = parts[2] if len(parts) > 2 else ''
        
        evaluation_results_third.append({
            'æ ‡ç­¾ç±»å‹': tag_type_name,
            'å®Œæ•´æ ‡ç­¾': label,
            'ä¸€çº§æ ‡ç­¾': domain_part,
            'äºŒçº§æ ‡ç­¾': intent_part,
            'ä¸‰çº§æ ‡ç­¾': third_part,
            'æ ·æœ¬æ•°é‡': label_count,
            'å‡†ç¡®ç‡': round(recall, 4),
            'ç²¾ç¡®ç‡': round(precision, 4),
            'å¬å›ç‡': round(recall, 4),
            'F1åˆ†æ•°': round(f1, 4)
        })
    
    results_df_three_level = pd.DataFrame(evaluation_results_third)
    
    # è®¡ç®—å„å±‚çº§çš„Macroå¹³å‡æŒ‡æ ‡
    two_level_macro_precision = results_df_two_level['ç²¾ç¡®ç‡'].mean() if not results_df_two_level.empty else 0
    two_level_macro_recall = results_df_two_level['å¬å›ç‡'].mean() if not results_df_two_level.empty else 0
    two_level_macro_f1 = results_df_two_level['F1åˆ†æ•°'].mean() if not results_df_two_level.empty else 0
    
    three_level_macro_precision = results_df_three_level['ç²¾ç¡®ç‡'].mean() if not results_df_three_level.empty else 0
    three_level_macro_recall = results_df_three_level['å¬å›ç‡'].mean() if not results_df_three_level.empty else 0
    three_level_macro_f1 = results_df_three_level['F1åˆ†æ•°'].mean() if not results_df_three_level.empty else 0
    
    # ä¸€çº§æ ‡ç­¾ï¼ˆé¢†åŸŸï¼‰çš„Macroå¹³å‡
    unique_domain_labels = sorted(df_filtered[corrected_domain_col].unique())
    domain_metrics = []
    
    for domain_label in unique_domain_labels:
        y_true_binary = (df_filtered[corrected_domain_col] == domain_label).astype(int)
        y_pred_binary = (df_filtered[domain_col] == domain_label).astype(int)
        
        precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
        recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
        f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
        domain_metrics.append({'precision': precision, 'recall': recall, 'f1': f1})
    
    domain_macro_precision = np.mean([m['precision'] for m in domain_metrics]) if domain_metrics else 0
    domain_macro_recall = np.mean([m['recall'] for m in domain_metrics]) if domain_metrics else 0
    domain_macro_f1 = np.mean([m['f1'] for m in domain_metrics]) if domain_metrics else 0
    
    # äºŒçº§æ ‡ç­¾ï¼ˆæ„å›¾ï¼‰çš„Macroå¹³å‡
    unique_intent_labels = sorted(df_filtered[corrected_intent_col].unique())
    intent_metrics = []
    
    for intent_label in unique_intent_labels:
        y_true_binary = (df_filtered[corrected_intent_col] == intent_label).astype(int)
        y_pred_binary = (df_filtered[intent_col] == intent_label).astype(int)
        
        precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
        recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
        f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
        intent_metrics.append({'precision': precision, 'recall': recall, 'f1': f1})
    
    intent_macro_precision = np.mean([m['precision'] for m in intent_metrics]) if intent_metrics else 0
    intent_macro_recall = np.mean([m['recall'] for m in intent_metrics]) if intent_metrics else 0
    intent_macro_f1 = np.mean([m['f1'] for m in intent_metrics]) if intent_metrics else 0
    
    # ä¸‰çº§æ ‡ç­¾ï¼ˆæ§½ä½ï¼‰çš„Macroå¹³å‡
    unique_third_labels = sorted(df_filtered[corrected_third_level_col].unique())
    third_metrics = []
    
    for third_label in unique_third_labels:
        y_true_binary = (df_filtered[corrected_third_level_col] == third_label).astype(int)
        y_pred_binary = (df_filtered[third_level_col] == third_label).astype(int)
        
        precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
        recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
        f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
        third_metrics.append({'precision': precision, 'recall': recall, 'f1': f1})
    
    third_macro_precision = np.mean([m['precision'] for m in third_metrics]) if third_metrics else 0
    third_macro_recall = np.mean([m['recall'] for m in third_metrics]) if third_metrics else 0
    third_macro_f1 = np.mean([m['f1'] for m in third_metrics]) if third_metrics else 0
    
    # æ„å»ºæ€»ä½“ç»Ÿè®¡è¡¨æ•°æ®
    summary_rows = [
        {
            'æ ‡ç­¾å±‚çº§': f'{tag_type_name}é¢†åŸŸæ„å›¾',
            'å‡†ç¡®ç‡': round(overall_accuracy, 4),
            'ç²¾ç¡®ç‡': round(two_level_macro_precision, 4),
            'å¬å›ç‡': round(two_level_macro_recall, 4),
            'F1åˆ†æ•°': round(two_level_macro_f1, 4)
        },
        {
            'æ ‡ç­¾å±‚çº§': f'{tag_type_name}é¢†åŸŸæ„å›¾æ§½ä½',
            'å‡†ç¡®ç‡': round(overall_three_level_accuracy, 4),
            'ç²¾ç¡®ç‡': round(three_level_macro_precision, 4),
            'å¬å›ç‡': round(three_level_macro_recall, 4),
            'F1åˆ†æ•°': round(three_level_macro_f1, 4)
        },
        {
            'æ ‡ç­¾å±‚çº§': f'{tag_type_name}ä¸€çº§',
            'å‡†ç¡®ç‡': round(domain_accuracy, 4),
            'ç²¾ç¡®ç‡': round(domain_macro_precision, 4),
            'å¬å›ç‡': round(domain_macro_recall, 4),
            'F1åˆ†æ•°': round(domain_macro_f1, 4)
        },
        {
            'æ ‡ç­¾å±‚çº§': f'{tag_type_name}äºŒçº§',
            'å‡†ç¡®ç‡': round(intent_accuracy, 4),
            'ç²¾ç¡®ç‡': round(intent_macro_precision, 4),
            'å¬å›ç‡': round(intent_macro_recall, 4),
            'F1åˆ†æ•°': round(intent_macro_f1, 4)
        },
        {
            'æ ‡ç­¾å±‚çº§': f'{tag_type_name}ä¸‰çº§',
            'å‡†ç¡®ç‡': round(third_level_accuracy, 4),
            'ç²¾ç¡®ç‡': round(third_macro_precision, 4),
            'å¬å›ç‡': round(third_macro_recall, 4),
            'F1åˆ†æ•°': round(third_macro_f1, 4)
        }
    ]
    
    return {
        'results_df_two_level': results_df_two_level,
        'results_df_three_level': results_df_three_level,
        'summary_rows': summary_rows
    }


def evaluate_single_level_tags(df, tag_config):
    """
    é€šç”¨å‡½æ•°ï¼šè¯„ä¼°å•çº§æ ‡ç­¾ï¼ˆå¦‚å’Œè§£çŠ¶æ€ï¼‰
    
    å‚æ•°:
        df: DataFrameï¼ŒåŒ…å«é¢„æµ‹å’Œä¿®æ­£æ ‡ç­¾çš„æ•°æ®
        tag_config: å­—å…¸ï¼ŒåŒ…å«æ ‡ç­¾é…ç½®ä¿¡æ¯
    
    è¿”å›:
        dict: åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    tag_type_name = tag_config['tag_type_name']
    pred_col = tag_config['pred_col']
    corrected_col = tag_config['corrected_col']
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = [pred_col, corrected_col]
    
    for col in required_columns:
        if col not in df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {col}")
    
    # è¿‡æ»¤æ‰æ²¡æœ‰äººå·¥ä¿®æ­£æ ‡ç­¾çš„æ•°æ®
    df_filtered = df[df[corrected_col].notna()].copy()
    
    if len(df_filtered) == 0:
        print(f"è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ç»è¿‡äººå·¥ä¿®æ­£çš„{tag_type_name}æ ‡ç­¾æ•°æ®")
        return None
    
    print(f"ä½¿ç”¨ {len(df_filtered)} æ¡ç»è¿‡äººå·¥ä¿®æ­£çš„{tag_type_name}æ ‡ç­¾æ•°æ®è¿›è¡Œè¯„ä¼°")
    
    # å¤„ç†ç©ºå€¼
    df_filtered.loc[:, pred_col] = df_filtered[pred_col].fillna('')
    df_filtered.loc[:, corrected_col] = df_filtered[corrected_col].fillna('')
    
    # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
    accuracy = (df_filtered[pred_col] == df_filtered[corrected_col]).mean()
    
    print(f"{tag_type_name}æ ‡ç­¾æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # è¯¦ç»†è¯„ä¼°
    unique_true_labels = sorted(df_filtered[corrected_col].unique())
    evaluation_results = []
    
    for label in unique_true_labels:
        label_mask = df_filtered[corrected_col] == label
        label_count = label_mask.sum()
        
        if label_count == 0:
            continue
        
        y_true_binary = (df_filtered[corrected_col] == label).astype(int)
        y_pred_binary = (df_filtered[pred_col] == label).astype(int)
        
        precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
        recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
        f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
        
        evaluation_results.append({
            'æ ‡ç­¾ç±»å‹': tag_type_name,
            'æ ‡ç­¾': label,
            'æ ·æœ¬æ•°é‡': label_count,
            'å‡†ç¡®ç‡': round(recall, 4),
            'ç²¾ç¡®ç‡': round(precision, 4),
            'å¬å›ç‡': round(recall, 4),
            'F1åˆ†æ•°': round(f1, 4)
        })
    
    results_df = pd.DataFrame(evaluation_results)
    
    # è®¡ç®—Macroå¹³å‡
    macro_precision = results_df['ç²¾ç¡®ç‡'].mean() if not results_df.empty else 0
    macro_recall = results_df['å¬å›ç‡'].mean() if not results_df.empty else 0
    macro_f1 = results_df['F1åˆ†æ•°'].mean() if not results_df.empty else 0
    
    # æ„å»ºæ€»ä½“ç»Ÿè®¡è¡¨æ•°æ®
    summary_rows = [
        {
            'æ ‡ç­¾å±‚çº§': tag_type_name,
            'å‡†ç¡®ç‡': round(accuracy, 4),
            'ç²¾ç¡®ç‡': round(macro_precision, 4),
            'å¬å›ç‡': round(macro_recall, 4),
            'F1åˆ†æ•°': round(macro_f1, 4)
        }
    ]
    
    return {
        'results_df': results_df,
        'summary_rows': summary_rows
    }


def evaluate_tagged_data(file_path, output_dir=None):
    """
    ä¸»å‡½æ•°ï¼šè¯„ä¼°æ‰€æœ‰æ ‡ç­¾ç±»å‹å¹¶ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    
    åŠŸèƒ½:
    - è¯„ä¼°è¯‰ç‚¹ã€è¯‰æ±‚ã€è§£å†³æ–¹æ¡ˆï¼ˆå¤šçº§æ ‡ç­¾ï¼‰
    - è¯„ä¼°å’Œè§£çŠ¶æ€ï¼ˆå•çº§æ ‡ç­¾ï¼‰
    - ç”Ÿæˆ *_all_evaluation.xlsx æŠ¥å‘Š
    
    å‚æ•°:
        file_path: è¾“å…¥Excelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        output_dir: å¯é€‰ï¼ŒæŒ‡å®šè¾“å‡ºç›®å½•ã€‚å¦‚æœä¸æŒ‡å®šï¼Œåˆ™ä¿å­˜åœ¨è¾“å…¥æ–‡ä»¶åŒç›®å½•
    
    è¿”å›:
        str: è¾“å‡ºExcelæ–‡ä»¶çš„è·¯å¾„
    """
    print(f"æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶: {file_path}")
    df = pd.read_excel(file_path)
    print(f"æ•°æ®è¯»å–å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•\n")

    # æ£€æµ‹æ˜¯å¦ä¸ºå‰ç«¯è°ƒç”¨
    is_frontend_call = 'Temp' in file_path and 'uploaded_tagged_' in file_path

    # ç”¨äºå­˜å‚¨æ‰€æœ‰æ ‡ç­¾ç±»å‹çš„è¯„ä¼°ç»“æœ
    all_results = {}

    # è¯„ä¼°è¯‰ç‚¹æ ‡ç­¾
    print("=" * 60)
    print("å¼€å§‹è¯„ä¼°è¯‰ç‚¹æ ‡ç­¾...")
    print("=" * 60)
    try:
        complaint_config = {
            'tag_type_name': 'è¯‰ç‚¹',
            'domain_col': 'complaint_domain',
            'intent_col': 'complaint_intent',
            'third_level_col': 'complaint_third_level',
            'corrected_domain_col': 'corrected_complaint_domain',
            'corrected_intent_col': 'corrected_complaint_intent',
            'corrected_third_level_col': 'corrected_complaint_third_level'
        }
        complaint_result = evaluate_multi_level_tags(df, complaint_config)
        if complaint_result:
            all_results['complaint'] = complaint_result
    except Exception as e:
        print(f"è¯‰ç‚¹æ ‡ç­¾è¯„ä¼°å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

    # è¯„ä¼°è¯‰æ±‚æ ‡ç­¾
    print("\n" + "=" * 60)
    print("å¼€å§‹è¯„ä¼°è¯‰æ±‚æ ‡ç­¾...")
    print("=" * 60)
    try:
        appeal_config = {
            'tag_type_name': 'è¯‰æ±‚',
            'domain_col': 'appeal_domain',
            'intent_col': 'appeal_intent',
            'third_level_col': 'appeal_third_level',
            'corrected_domain_col': 'corrected_appeal_domain',
            'corrected_intent_col': 'corrected_appeal_intent',
            'corrected_third_level_col': 'corrected_appeal_third_level'
        }
        appeal_result = evaluate_multi_level_tags(df, appeal_config)
        if appeal_result:
            all_results['appeal'] = appeal_result
    except Exception as e:
        print(f"è¯‰æ±‚æ ‡ç­¾è¯„ä¼°å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

    # è¯„ä¼°è§£å†³æ–¹æ¡ˆæ ‡ç­¾
    print("\n" + "=" * 60)
    print("å¼€å§‹è¯„ä¼°è§£å†³æ–¹æ¡ˆæ ‡ç­¾...")
    print("=" * 60)
    try:
        solution_config = {
            'tag_type_name': 'è§£å†³æ–¹æ¡ˆ',
            'domain_col': 'solution_domain',
            'intent_col': 'solution_intent',
            'third_level_col': 'solution_third_level',
            'corrected_domain_col': 'corrected_solution_domain',
            'corrected_intent_col': 'corrected_solution_intent',
            'corrected_third_level_col': 'corrected_solution_third_level'
        }
        solution_result = evaluate_multi_level_tags(df, solution_config)
        if solution_result:
            all_results['solution'] = solution_result
    except Exception as e:
        print(f"è§£å†³æ–¹æ¡ˆæ ‡ç­¾è¯„ä¼°å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

    # è¯„ä¼°å’Œè§£çŠ¶æ€æ ‡ç­¾
    print("\n" + "=" * 60)
    print("å¼€å§‹è¯„ä¼°å’Œè§£çŠ¶æ€æ ‡ç­¾...")
    print("=" * 60)
    try:
        reconciliation_config = {
            'tag_type_name': 'å’Œè§£çŠ¶æ€',
            'pred_col': 'reconciliation_status',
            'corrected_col': 'corrected_reconciliation_status'
        }
        reconciliation_result = evaluate_single_level_tags(df, reconciliation_config)
        if reconciliation_result:
            all_results['reconciliation'] = reconciliation_result
    except Exception as e:
        print(f"å’Œè§£çŠ¶æ€æ ‡ç­¾è¯„ä¼°å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
    input_filename = os.path.splitext(os.path.basename(file_path))[0]
    
    if is_frontend_call:
        # å‰ç«¯è°ƒç”¨ï¼šç›´æ¥ä¿å­˜åˆ° evaluation_reports ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))  # scripts ç›®å½•
        project_root = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•
        eval_reports_dir = os.path.join(project_root, 'evaluation_reports')
        os.makedirs(eval_reports_dir, exist_ok=True)
        
        # ç”Ÿæˆå‰ç«¯æœŸæœ›çš„æ–‡ä»¶åæ ¼å¼
        import uuid
        report_filename = f"evaluation_{uuid.uuid4().hex}.xlsx"
        output_path = os.path.join(eval_reports_dir, report_filename)
        print(f"æ£€æµ‹åˆ°å‰ç«¯è°ƒç”¨ï¼Œç›´æ¥ä¿å­˜åˆ°: {eval_reports_dir}")
    elif output_dir:
        # æŒ‡å®šäº†è¾“å‡ºç›®å½•
        output_folder = output_dir
        os.makedirs(output_folder, exist_ok=True)
        output_path = os.path.join(output_folder, f'{input_filename}_all_evaluation.xlsx')
    else:
        # é»˜è®¤ï¼šä¿å­˜åœ¨è¾“å…¥æ–‡ä»¶åŒç›®å½•
        output_folder = os.path.dirname(file_path)
        output_path = os.path.join(output_folder, f'{input_filename}_all_evaluation.xlsx')
    
    print(f"\næ­£åœ¨ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path}")

    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡Excelæ–‡ä»¶
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        # åˆå¹¶æ‰€æœ‰æ€»ä½“ç»Ÿè®¡æ•°æ®
        all_summary_rows = []
        
        for tag_key, result in all_results.items():
            if 'summary_rows' in result:
                all_summary_rows.extend(result['summary_rows'])
        
        # ä¿å­˜æ€»ä½“ç»Ÿè®¡Sheet
        if all_summary_rows:
            summary_df = pd.DataFrame(all_summary_rows)
            summary_df.to_excel(writer, sheet_name='æ€»ä½“ç»Ÿè®¡', index=False)

        # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœSheet
        if 'complaint' in all_results and 'results_df_two_level' in all_results['complaint']:
            all_results['complaint']['results_df_two_level'].to_excel(
                writer, sheet_name='è¯‰ç‚¹-é¢†åŸŸæ„å›¾', index=False)

        if 'appeal' in all_results and 'results_df_two_level' in all_results['appeal']:
            all_results['appeal']['results_df_two_level'].to_excel(
                writer, sheet_name='è¯‰æ±‚-é¢†åŸŸæ„å›¾', index=False)

        if 'solution' in all_results and 'results_df_two_level' in all_results['solution']:
            all_results['solution']['results_df_two_level'].to_excel(
                writer, sheet_name='è§£å†³æ–¹æ¡ˆ-é¢†åŸŸæ„å›¾', index=False)

        if 'reconciliation' in all_results and 'results_df' in all_results['reconciliation']:
            all_results['reconciliation']['results_df'].to_excel(
                writer, sheet_name='å’Œè§£çŠ¶æ€', index=False)

        if 'complaint' in all_results and 'results_df_three_level' in all_results['complaint']:
            all_results['complaint']['results_df_three_level'].to_excel(
                writer, sheet_name='è¯‰ç‚¹-é¢†åŸŸæ„å›¾æ§½ä½', index=False)

        if 'appeal' in all_results and 'results_df_three_level' in all_results['appeal']:
            all_results['appeal']['results_df_three_level'].to_excel(
                writer, sheet_name='è¯‰æ±‚-é¢†åŸŸæ„å›¾æ§½ä½', index=False)

        if 'solution' in all_results and 'results_df_three_level' in all_results['solution']:
            all_results['solution']['results_df_three_level'].to_excel(
                writer, sheet_name='è§£å†³æ–¹æ¡ˆ-é¢†åŸŸæ„å›¾æ§½ä½', index=False)

    print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {output_path}")
    print("\n" + "=" * 60)
    print("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print("=" * 60)
    
    return output_path


def main():
    """
    ä¸»ç¨‹åºå…¥å£å‡½æ•°
    """
    # é»˜è®¤æ–‡ä»¶è·¯å¾„
    file_path = r"C:\Users\T14P\Desktop\benchmark_1800_taggedv1.5_promptv1.3.xlsx"

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        print(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ–‡ä»¶è·¯å¾„: {file_path}")
    else:
        print(f"ä½¿ç”¨é»˜è®¤æ–‡ä»¶è·¯å¾„: {file_path}")

    try:
        result_path = evaluate_tagged_data(file_path)
        
        if result_path:
            print(f"\nğŸ“Š ä¸»è¦ç»“æœæ–‡ä»¶: {result_path}")
            print(f"ğŸ“ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°: {os.path.dirname(result_path)}")
        else:
            print("âŒ è¯„ä¼°å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶å’Œæ•°æ®æ ¼å¼")
            
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {file_path}")
        return 1
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())